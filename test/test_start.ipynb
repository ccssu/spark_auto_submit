{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from spark_auto_submit_sdk.sdk import SparkAutoSubmitSDK\n",
    "\n",
    "# # 创建 SparkAutoSubmitSDK 实例\n",
    "config_file = '/workspace/DEMO/development_local.yaml'\n",
    "sdk = SparkAutoSubmitSDK(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建提交参数，指定应用程序名称、主文件和参数\n",
    "app_name = \"MySparkApp\"\n",
    "main_file = \"/workspace/DEMO/pyspark-corpus-pipeline-demo/application.py\"\n",
    "# application.py --idf_path=./data/idf.txt --idx_path=./data/idx.txt --model_path=./data/model --raw_path=./data/sample_toutiao_cat_data.txt --char_emb_path=./data/char_emb.txt --result_path=result.txt\n",
    "args = [{\"--idf_path\":\"/workspace/DEMO/pyspark-corpus-pipeline-demo/data/idf.txt\"},\n",
    "        {\"--idx_path\":\"/workspace/DEMO/pyspark-corpus-pipeline-demo/data/idx.txt\"},\n",
    "        {\"--model_path\":\"/workspace/DEMO/pyspark-corpus-pipeline-demo/data/model\"},\n",
    "        {\"--raw_path\":\"/workspace/DEMO/pyspark-corpus-pipeline-demo/data/sample_toutiao_cat_data.txt\"},\n",
    "        {\"--char_emb_path\":\"/workspace/DEMO/pyspark-corpus-pipeline-demo/data/char_emb.txt\"},\n",
    "        {\"--result_path\":\"result.txt\"}\n",
    "        ]\n",
    "# TODO(fengwen) 修复：args 传入参数时， --args 不能省略 , 如果只是 --no-save 没有后面的参数\n",
    "submission_params = sdk.create_submission_parameters(app_name, main_file, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdk.submit_application(submission_params)\n",
    "print(\"submission_params: \", submission_params.to_spark_submit_command())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from spark_auto_submit_sdk.core.default_zip_manager import DefaultZipManager\n",
    "from spark_auto_submit_sdk.core.utils.default_utils import working_directory\n",
    "\n",
    "directory_path = \"/opt/conda/envs/empty\"\n",
    "\n",
    "zip_manager = DefaultZipManager(directory_path)\n",
    "output_path = \"/opt/conda/envs/empty.zip\"\n",
    "zip_path = zip_manager.zip_directory(directory_path,output_path, \"empty.zip\")\n",
    "print(\"Zip file created:\", zip_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from spark_auto_submit_sdk.core.ssh_manager import SSHNode,ProxyNode\n",
    "# - IP: 10.0.72.96\n",
    "# - user: `guangnian`\n",
    "# - password: `data_test@GN23`\n",
    "\n",
    "with SSHNode('10.0.72.96', 22,'guangnian', 'data_test@GN23') as node:\n",
    "    # execute_callback = lambda x : print(x.get('stdout').decode('utf-8'))\n",
    "    # node.transfer_data(zip_path, '/data/guangnian/temp')\n",
    "    # # result = node.transfer_data('/workspace/DEMO/test_project00/pyspark_venv.tar.gz', '/data/guangnian/temp')\n",
    "    # # destination_file = '/data/guangnian/temp/pyspark_venv.tar.gz'\n",
    "    # # node.execute_command('ls -l /data/guangnian/temp', execute_callback)\n",
    "    # destination_file = '/data/guangnian/temp/pyspark_venv.zip'\n",
    "    # base_name = os.path.basename(destination_file)\n",
    "    # hadoop = '/usr/local/service/hadoop/bin/hadoop'\n",
    "    # node.execute_command(f'{hadoop} fs -put {destination_file} /py_env/{base_name}', execute_callback)\n",
    "    \n",
    "    # # node.execute_command(f'{hadoop} fs -ls /py_env/', execute_callback)\n",
    "\n",
    "    node.transfer_data('/opt/conda/envs/empty.zip','/data/guangnian/temp')\n",
    "    node.transfer_data('/workspace/DEMO/test_project00/app.py','/data/guangnian/temp')\n",
    "\n",
    "    # node.execute_command(f'rm -rf {destination_file}', execute_callback)\n",
    "    hadoop = '/usr/local/service/hadoop/bin/hadoop'\n",
    "    node.execute_command(f'{hadoop} fs -put /data/guangnian/temp/empty.zip /py_env/empty.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_tool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
