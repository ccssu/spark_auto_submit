{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing SparkAutoSubmitSDK...\n",
      "2023-06-12 19:41:29,567 - INFO - Initializing SparkAutoSubmitSDK...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_mode:  yarn\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from spark_auto_submit_sdk.sdk import SparkAutoSubmitSDK\n",
    "\n",
    "# # 创建 SparkAutoSubmitSDK 实例\n",
    "config_file = '/workspace/DEMO/development_local.yaml'\n",
    "sdk = SparkAutoSubmitSDK(config_file)\n",
    "# ImportError: cannot import name 'SparkAutoSubmitSDK' from 'spark_auto_submit_sdk' (/opt/conda/envs/spark_tool/lib/python3.8/site-packages/spark_auto_submit_sdk/__init__.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark_auto_submit_sdk.scheduling.strategy:Performing Remote scheduling.\n",
      "2023-06-12 19:41:29,650 - INFO - Performing Remote scheduling.\n",
      "INFO:spark_auto_submit_sdk.scheduling.strategy:Scheduling task: package_task\n",
      "2023-06-12 19:41:29,651 - INFO - Scheduling task: package_task\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackageTask run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "压缩项目代码.zip: 100%|██████████| 3/3 [00:00<00:00, 801.87file(s)/s]\n",
      "INFO:spark_auto_submit_sdk.core.utils.default_utils:Function package_task executed in 0.037603139877319336 seconds\n",
      "2023-06-12 19:41:29,690 - INFO - Function package_task executed in 0.037603139877319336 seconds\n",
      "INFO:spark_auto_submit_sdk.scheduling.strategy:Performing Remote scheduling.\n",
      "2023-06-12 19:41:29,690 - INFO - Performing Remote scheduling.\n",
      "INFO:spark_auto_submit_sdk.scheduling.strategy:Scheduling task: build_command_task\n",
      "2023-06-12 19:41:29,691 - INFO - Scheduling task: build_command_task\n",
      "INFO:spark_auto_submit_sdk.core.utils.default_utils:Function build_command_task executed in 7.462501525878906e-05 seconds\n",
      "2023-06-12 19:41:29,692 - INFO - Function build_command_task executed in 7.462501525878906e-05 seconds\n"
     ]
    }
   ],
   "source": [
    "# 创建提交参数，指定应用程序名称、主文件和参数\n",
    "app_name = \"MySparkApp\"\n",
    "main_file = \"/workspace/DEMO/pyspark-corpus-pipeline-demo/application.py\"\n",
    "# application.py --idf_path=./data/idf.txt --idx_path=./data/idx.txt --model_path=./data/model --raw_path=./data/sample_toutiao_cat_data.txt --char_emb_path=./data/char_emb.txt --result_path=result.txt\n",
    "args = [{\"--idf_path\":\"/workspace/DEMO/pyspark-corpus-pipeline-demo/data/idf.txt\"},\n",
    "        {\"--idx_path\":\"/workspace/DEMO/pyspark-corpus-pipeline-demo/data/idx.txt\"},\n",
    "        {\"--model_path\":\"/workspace/DEMO/pyspark-corpus-pipeline-demo/data/model\"},\n",
    "        {\"--raw_path\":\"/workspace/DEMO/pyspark-corpus-pipeline-demo/data/sample_toutiao_cat_data.txt\"},\n",
    "        {\"--char_emb_path\":\"/workspace/DEMO/pyspark-corpus-pipeline-demo/data/char_emb.txt\"},\n",
    "        {\"--result_path\":\"result.txt\"}\n",
    "        ]\n",
    "# TODO(fengwen) 修复：args 传入参数时， --args 不能省略 , 如果只是 --no-save 没有后面的参数\n",
    "submission_params = sdk.create_submission_parameters(app_name, main_file, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_params:  spark-submit \\\n",
      "--deploy-mode cluster \\\n",
      "--master yarn \\\n",
      "--num-executors 10 \\\n",
      "--driver-memory 2g \\\n",
      "--executor-memory 2g \\\n",
      "--driver-memory 16g \\\n",
      "--conf spark.executor.memory=2g \\\n",
      "--conf spark.default.parallelism=50 \\\n",
      "--name MySparkApp \\\n",
      "--py-files /workspace/DEMO/test_project00.zip \\\n",
      "--archives /workspace/DEMO/test_project00.zip#PY3 \\\n",
      "--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./PY3/test_project00/bin/python \\\n",
      "/workspace/DEMO/pyspark-corpus-pipeline-demo/application.py \\\n",
      "--idf_path /workspace/DEMO/pyspark-corpus-pipeline-demo/data/idf.txt \\\n",
      "--idx_path /workspace/DEMO/pyspark-corpus-pipeline-demo/data/idx.txt \\\n",
      "--model_path /workspace/DEMO/pyspark-corpus-pipeline-demo/data/model \\\n",
      "--raw_path /workspace/DEMO/pyspark-corpus-pipeline-demo/data/sample_toutiao_cat_data.txt \\\n",
      "--char_emb_path /workspace/DEMO/pyspark-corpus-pipeline-demo/data/char_emb.txt \\\n",
      "--result_path result.txt\n"
     ]
    }
   ],
   "source": [
    "# sdk.submit_application(submission_params)\n",
    "print(\"submission_params: \", submission_params.to_spark_submit_command())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (581052238.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [5]\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from spark_auto_submit_sdk.core import SSHNode,ProxyNode\n",
    "# - IP: 10.0.72.96\n",
    "# - user: `guangnian`\n",
    "# - password: `data_test@GN23`\n",
    "\n",
    "with SSHNode('10.0.72.96', 22,'guangnian', 'data_test@GN23') as node:\n",
    "    # execute_callback = lambda x : print(x.get('stdout').decode('utf-8'))\n",
    "    # node.transfer_data(zip_path, '/data/guangnian/temp')\n",
    "    # # result = node.transfer_data('/workspace/DEMO/test_project00/pyspark_venv.tar.gz', '/data/guangnian/temp')\n",
    "    # # destination_file = '/data/guangnian/temp/pyspark_venv.tar.gz'\n",
    "    # # node.execute_command('ls -l /data/guangnian/temp', execute_callback)\n",
    "    # destination_file = '/data/guangnian/temp/pyspark_venv.zip'\n",
    "    # base_name = os.path.basename(destination_file)\n",
    "    # hadoop = '/usr/local/service/hadoop/bin/hadoop'\n",
    "    # node.execute_command(f'{hadoop} fs -put {destination_file} /py_env/{base_name}', execute_callback)\n",
    "    # # node.execute_command(f'{hadoop} fs -ls /py_env/', execute_callback)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_tool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
